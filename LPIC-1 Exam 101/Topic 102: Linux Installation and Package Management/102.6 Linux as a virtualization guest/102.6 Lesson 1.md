# 102.6 Урок 1

| **Сертификат:** | LPIC-1                                    |
|:----------------|:------------------------------------------|
| **Версия:**     | 5.0                                       |
| **Тема:**       | 102 Установка Linux и управление пакетами |
| **Цель:**       | 102.6 Linux как гость виртуализации       | 
| **Урок:**       | 1 из 1                                    |


## Введение

Одна из самых сильных сторон Linux - его универсальность. Одним из аспектов этой универсальности является возможность использовать Linux как средство размещения других операционных систем или отдельных приложений в полностью изолированной и безопасной среде. Этот урок будет посвящен концепциям виртуализации и контейнерных технологий, а также некоторым техническим деталям, которые следует учитывать при развертывании виртуальной машины на облачной платформе.


## Обзор виртуализации

Виртуализация - это технология, которая позволяет программной платформе, называемой `гипервизором`, запускать процессы, содержащие полностью эмулируемую компьютерную систему. Гипервизор отвечает за управление ресурсами физического оборудования, которые могут использоваться отдельными виртуальными машинами. Эти виртуальные машины называются `гостями` гипервизора. Виртуальная машина имеет многие аспекты физического компьютера, эмулируемые в программном обеспечении, например BIOS системы и контроллеры жестких дисков. Виртуальная машина часто использует образы жестких дисков, которые хранятся в виде отдельных файлов, и будет иметь доступ к RAM и CPU хост-машины через программное обеспечение гипервизора. Гипервизор разделяет доступ к аппаратным ресурсам хост-системы между гостевыми системами, что позволяет нескольким операционным системам работать на одной хост-системе. 

Обычно используемые гипервизоры для Linux включают: 

Xen  
Xen - это гипервизор Type-1 с открытым исходным кодом, что означает, что он не полагается на базовую операционную систему в своей работе. Гипервизор такого типа известен как `гипервизор «голого железа»`, поскольку компьютер может загружаться непосредственно в гипервизор. 

KVM  
Виртуальная машина ядра - это модуль ядра Linux для виртуализации. KVM - это гипервизор обоих типов как Type-1, так и Type-2, потому что, хоть и для работы ему требуется общая операционная система Linux, он может отлично работать как гипервизор за счет интеграции с установленной ОС Linux. Виртуальные машины, развернутые с помощью KVM, используют демон `libvirt` и связанные с ним программные утилиты для создания и управления. 

VirtualBox  
Популярное настольное приложение, упрощающее создание виртуальных машин и управление ими. Oracle VM VirtualBox является кроссплатформенным и будет работать в Linux, macOS и Microsoft Windows. Поскольку для работы VirtualBox требуется базовая операционная система, это гипервизор Type-2. 

Некоторые гипервизоры позволяют динамически перемещать виртуальную машину. Процесс перемещения виртуальной машины с одного гипервизора на другой называется *миграцией*, и используемые методы различаются в зависимости от реализации гипервизора. Некоторые миграции могут быть выполнены только после того, как гостевая система полностью выключена, а другие могут быть выполнены во время работы гостевой системы (это называется *динамической миграцией*). Такие методы могут быть полезны во время периодов обслуживания гипервизоров или для обеспечения отказоустойчивости системы, когда гипервизор становится нефункциональным, а гостя можно переместить на работающий гипервизор.


## Типы виртуальных машин

Существует три основных типа виртуальных машин: *полностью виртуализированный* гость, *паравиртуализированный* гость и *гибридный* гость. 

Полностью виртуализированный  
Все инструкции, которые должна выполнять гостевая операционная система, должны иметь возможность запускаться в полностью виртуализированной установке операционной системы. Причина этого в том, что в гостевой системе не устанавливаются дополнительные программные драйверы для перевода инструкций на смоделированное или реальное оборудование. Полностью виртуализированный гость - это гость, в которой гость (или HardwareVM) не знает, что это запущенный экземпляр виртуальной машины. Чтобы этот тип виртуализации имел место на оборудовании на базе x86, в системе с установленным гипервизором должны быть включены расширения CPU Intel VT-x или AMD-V. Это можно сделать из меню конфигурации прошивки BIOS или UEFI. 

Паравиртуализированный  
Паравиртуализированный гость (или PVM) - это гость, в которой гостевая операционная система знает, что это запущенный экземпляр виртуальной машины. Эти типы гостевых систем будут использовать модифицированное ядро и специальные драйверы (известные как гостевые драйверы), которые помогут гостевой операционной системе использовать программные и аппаратные ресурсы гипервизора. Производительность паравиртуализированной гостевой системы часто лучше, чем у полностью виртуализированной гостевой системы, благодаря преимуществам, которые предоставляют эти программные драйверы. 

Гибридный  
Паравиртуализация и полная виртуализация могут быть объединены, чтобы позволить немодифицированным операционным системам получить производительность ввода-вывода, близкую к исходной, за счет использования паравиртуализированных драйверов в полностью виртуализированных операционных системах. Паравиртуализированные драйверы содержат драйверы хранилищ и сетевых устройств с улучшенной производительностью дискового и сетевого ввода-вывода. 

Платформы виртуализации часто предоставляют упакованные гостевые драйверы для виртуализированных операционных систем. KVM использует драйверы из проекта Virtio, тогда как Oracle VM VirtualBox использует гостевые расширения, доступные из загружаемого файла образа ISO CD-ROM.


## Пример виртуальной машины `libvirt`

Мы рассмотрим пример виртуальной машины, которая управляется `libvirt` и использует гипервизор KVM. Виртуальная машина часто состоит из группы файлов, в первую очередь XML-файла, который *определяет* виртуальную машину (например, ее аппаратную конфигурацию, сетевое подключение, возможности отображения и т. д.) И связанного файла образа жесткого диска, который содержит установку операционной системы. система и ее программное обеспечение. 

Во-первых, давайте начнем исследовать пример XML-файла конфигурации для виртуальной машины и ее сетевой среды:

```console
$ ls /etc/libvirt/qemu
total 24
drwxr-xr-x 3 root root 4096 Oct 29 17:48 networks
-rw------- 1 root root 5667 Jun 29 17:17 rhel8.0.xml
```

>Часть пути к каталогу `qemu` относится к базовому программному обеспечению, на которое полагаются виртуальные машины на основе KVM. Проект QEMU предоставляет программное обеспечение для гипервизора для эмуляции аппаратных устройств, которые будет использовать виртуальная машина, таких как контроллеры дисков, доступ к центральному процессору, эмуляция сетевой карты и многое другое.

Обратите внимание, что есть каталог с именем `networks`. Этот каталог содержит файлы определений (также использующие XML), которые создают сетевые конфигурации, которые могут использовать виртуальные машины. Этот гипервизор использует только одну сеть, поэтому существует только один файл определения, который содержит конфигурацию для сегмента виртуальной сети, который эти системы будут использовать.

```console
$ ls -l /etc/libvirt/qemu/networks/
total 8
drwxr-xr-x 2 root root 4096 Jun 29 17:15 autostart
-rw------- 1 root root  576 Jun 28 16:39 default.xml
$ sudo cat /etc/libvirt/qemu/networks/default.xml
<!--
WARNING: THIS IS AN AUTO-GENERATED FILE. CHANGES TO IT ARE LIKELY TO BE
OVERWRITTEN AND LOST. Changes to this xml configuration should be made using:
  virsh net-edit default
or other application using the libvirt API.
-->

<network>
  <name>default</name>
  <uuid>55ab064f-62f8-49d3-8d25-8ef36a524344</uuid>
  <forward mode='nat'/>
  <bridge name='virbr0' stp='on' delay='0'/>
  <mac address='52:54:00:b8:e0:15'/>
  <ip address='192.168.122.1' netmask='255.255.255.0'>
    <dhcp>
      <range start='192.168.122.2' end='192.168.122.254'/>
    </dhcp>
  </ip>
</network>
```

Это определение включает частную сеть класса C и эмулируемое аппаратное устройство, выступающее в качестве маршрутизатора для этой сети. Также существует диапазон IP-адресов для использования DHCP-сервером гипервизора, которые могут быть назначены виртуальным машинам, использующим эту сеть. Эта сетевая конфигурация также использует *преобразование сетевых адресов* (NAT) для пересылки пакетов в другие сети, такие как LAN гипервизора. 

Теперь обратим внимание на файл определения виртуальной машины Red Hat Enterprise Linux 8. (разделы особого примечания выделены жирным шрифтом):

```console
$ sudo cat /etc/libvirt/qemu/rhel8.0.xml
<!--
WARNING: THIS IS AN AUTO-GENERATED FILE. CHANGES TO IT ARE LIKELY TO BE
OVERWRITTEN AND LOST. Changes to this xml configuration should be made using:
  virsh edit rhel8.0
or other application using the libvirt API.
-->

<domain type='kvm'>
  <name>rhel8.0</name>
  <uuid>fadd8c5d-c5e1-410e-b425-30da7598d0f6</uuid>
  <metadata>
    <libosinfo:libosinfo xmlns:libosinfo="http://libosinfo.org/xmlns/libvirt/domain/1.0">
      <libosinfo:os id="http://redhat.com/rhel/8.0"/>
    </libosinfo:libosinfo>
  </metadata>
  <memory unit='KiB'>4194304</memory>
  <currentMemory unit='KiB'>4194304</currentMemory>
  <vcpu placement='static'>2</vcpu>
  <os>
    <type arch='x86_64' machine='pc-q35-3.1'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <vmport state='off'/>
  </features>
  <cpu mode='host-model' check='partial'>
    <model fallback='allow'/>
  </cpu>
  <clock offset='utc'>
    <timer name='rtc' tickpolicy='catchup'/>
    <timer name='pit' tickpolicy='delay'/>
    <timer name='hpet' present='no'/>
  </clock>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>destroy</on_crash>
  <pm>
    <suspend-to-mem enabled='no'/>
    <suspend-to-disk enabled='no'/>
  </pm>
  <devices>
    <emulator>/usr/bin/qemu-system-x86_64</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2'/>
      <source file='/var/lib/libvirt/images/rhel8'/>
      <target dev='vda' bus='virtio'/>
      <address type='pci' domain='0x0000' bus='0x04' slot='0x00' function='0x0'/>
    </disk>
    <controller type='usb' index='0' model='qemu-xhci' ports='15'>
      <address type='pci' domain='0x0000' bus='0x02' slot='0x00' function='0x0'/>
    </controller>
    <controller type='sata' index='0'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x1f' function='0x2'/>
    </controller>
    <controller type='pci' index='0' model='pcie-root'/>
    <controller type='pci' index='1' model='pcie-root-port'>
      <model name='pcie-root-port'/>
      <target chassis='1' port='0x10'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0' multifunction='on'/>
    </controller>
    <controller type='pci' index='2' model='pcie-root-port'>
      <model name='pcie-root-port'/>
      <target chassis='2' port='0x11'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x1'/>
    </controller>
    <controller type='pci' index='3' model='pcie-root-port'>
      <model name='pcie-root-port'/>
      <target chassis='3' port='0x12'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x2'/>
    </controller>
    <controller type='pci' index='4' model='pcie-root-port'>
      <model name='pcie-root-port'/>
      <target chassis='4' port='0x13'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x3'/>
    </controller>
    <controller type='pci' index='5' model='pcie-root-port'>
      <model name='pcie-root-port'/>
      <target chassis='5' port='0x14'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x4'/>
    </controller>
    <controller type='pci' index='6' model='pcie-root-port'>
      <model name='pcie-root-port'/>
      <target chassis='6' port='0x15'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x5'/>
    </controller>
    <controller type='pci' index='7' model='pcie-root-port'>
      <model name='pcie-root-port'/>
      <target chassis='7' port='0x16'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x6'/>
    </controller>
    <controller type='virtio-serial' index='0'>
      <address type='pci' domain='0x0000' bus='0x03' slot='0x00' function='0x0'/>
    </controller>
    <interface type='network'>
      <mac address='52:54:00:50:a7:18'/>
      <source network='default'/>
      <model type='virtio'/>
      <address type='pci' domain='0x0000' bus='0x01' slot='0x00' function='0x0'/>
    </interface>
    <serial type='pty'>
      <target type='isa-serial' port='0'>
        <model name='isa-serial'/>
      </target>
    </serial>
    <console type='pty'>
      <target type='serial' port='0'/>
    </console>
    <channel type='unix'>
      <target type='virtio' name='org.qemu.guest_agent.0'/>
      <address type='virtio-serial' controller='0' bus='0' port='1'/>
    </channel>
    <channel type='spicevmc'>
      <target type='virtio' name='com.redhat.spice.0'/>
      <address type='virtio-serial' controller='0' bus='0' port='2'/>
    </channel>
    <input type='tablet' bus='usb'>
      <address type='usb' bus='0' port='1'/>
    </input>
    <input type='mouse' bus='ps2'/>
    <input type='keyboard' bus='ps2'/>
    <graphics type='spice' autoport='yes'>
      <listen type='address'/>
      <image compression='off'/>
    </graphics>
    <sound model='ich9'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x1b' function='0x0'/>
    </sound>
    <video>
      <model type='virtio' heads='1' primary='yes'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x0'/>
    </video>
    <redirdev bus='usb' type='spicevmc'>
      <address type='usb' bus='0' port='2'/>
    </redirdev>
    <redirdev bus='usb' type='spicevmc'>
      <address type='usb' bus='0' port='3'/>
    </redirdev>
    <memballoon model='virtio'>
      <address type='pci' domain='0x0000' bus='0x05' slot='0x00' function='0x0'/>
    </memballoon>
    <rng model='virtio'>
      <backend model='random'>/dev/urandom</backend>
      <address type='pci' domain='0x0000' bus='0x06' slot='0x00' function='0x0'/>
    </rng>
  </devices>
</domain>
```

Этот файл определяет ряд аппаратных настроек, которые используются этим гостем, такие как объем RAM, который он будет ему назначен, количество ядер CPU от гипервизора, к которому гость будет иметь доступ, файл образа жесткого диска. который связан с этим гостем (в разделе диска), его возможностями отображения (через протокол SPICE) и доступом гостя к USB-устройствам, а также эмулированным вводом с клавиатуры и мыши.


## Пример дискового хранилища виртуальной машины

Образ жесткого диска этой виртуальной машины находится в `/var/lib/libvirt/images/rhel8`. Вот сам образ диска на этом гипервизоре:

```console
$ sudo ls -lh /var/lib/libvirt/images/rhel8
-rw------- 1 root root 5.5G Oct 25 15:57 /var/lib/libvirt/images/rhel8
```

Текущий размер этого образа диска занимает всего 5,5 ГБ места на гипервизоре. Однако операционная система в этой гостевой системе видит диск размером 23,3 ГБ, о чем свидетельствует вывод следующей команды на работающей виртуальной машине:

```console
$ lsblk
NAME          MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
vda           252:0    0 23.3G  0 disk
├─vda1        252:1    0    1G  0 part /boot
└─vda2        252:2    0 22.3G  0 part
  ├─rhel-root 253:0    0   20G  0 lvm  /
  └─rhel-swap 253:1    0  2.3G  0 lvm  [SWAP]
```

Это связано с типом выделения диска, используемым для этого гостя. Существует несколько типов образов дисков, которые может использовать виртуальная машина, но два основных типа: 

COW  
Копирование при записи (также называемое *тонким предоставлением* или *разреженными образами*) - это метод, при котором файл на диске создается с заранее определенным верхним пределом размера. Размер образа диска увеличивается только по мере записи на диск новых данных. Как и в предыдущем примере, гостевая операционная система видит предустановленный дисковый предел в 23,3 ГБ, но записала в файл на диске только 5,5 ГБ данных. Формат образа диска, используемый для виртуальной машины в качестве примера, - `qcow2`, который представляет собой формат файла QEMU COW. 

RAW  
*raw* или *полный* тип диска - это файл, для которого предварительно выделено все пространство. Например, файл необработанного образа диска размером 10 ГБ занимает 10 ГБ фактического дискового пространства гипервизора. Этот стиль диска дает преимущество в производительности, поскольку все необходимое дисковое пространство уже существует, поэтому базовый гипервизор может просто записывать данные на диск без снижения производительности при мониторинге образа диска, чтобы убедиться, что он еще не достиг своего предела. и увеличение размера файла по мере записи в него новых данных. 

Существуют и другие платформы управления виртуализацией, такие как *Red Hat Enterprise Virtualization* и *oVirt*, которые могут использовать физические диски в качестве резервных хранилищ для операционной системы виртуальной машины. Эти системы могут использовать устройства хранения данных (SAN) или сетевые устройства хранения (NAS) для записи своих данных, а гипервизор отслеживает, какие места хранения принадлежат каким виртуальным машинам. Эти системы хранения могут использовать такие технологии, как управление логическими томами (LVM) для увеличения или уменьшения размера дискового хранилища виртуальной машины по мере необходимости, а также для помощи в создании моментальных снимков хранилища и управлении ими.


## Работа с шаблонами виртуальных машин

Поскольку виртуальные машины обычно представляют собой просто файлы, запущенные на гипервизоре, легко создавать шаблоны, которые можно настроить для конкретных сценариев развертывания. Часто на виртуальной машине будет установлена базовая операционная система и некоторые предварительно настроенные параметры конфигурации аутентификации, настроенные для облегчения будущих запусков системы. Это сокращает время, необходимое для создания новой системы, за счет уменьшения объема часто повторяемой работы, такой как установка базового пакета и настройки локали. 

Позднее этот шаблон виртуальной машины можно будет скопировать в новую гостевую систему. В этом случае новый гость будет переименован, для его сетевого интерфейса будет создан новый MAC-адрес, и в зависимости от его предполагаемого использования могут быть внесены другие изменения.


## Идентификатор машины D-Bus

Многие установки Linux будут использовать идентификационный номер машины, сгенерированный во время установки, который называется *идентификатором машины D-Bus*. Однако, если виртуальная машина *клонируется* для использования в качестве шаблона для других установок виртуальной машины, необходимо будет создать новый идентификатор машины D-Bus, чтобы гарантировать, что системные ресурсы из гипервизора будут направлены в соответствующую гостевую систему. 

Следующая команда может использоваться для проверки того, что идентификатор машины D-Bus существует для работающей системы:

```console
$ dbus-uuidgen --ensure
```

Если сообщения об ошибках не отображаются, значит, для системы существует идентификатор. Чтобы просмотреть текущий идентификатор машины D-Bus, запустите следующее:

```console
$ dbus-uuidgen --get
17f2e0698e844e31b12ccd3f9aa4d94a
```

Отображаемая строка текста является текущим идентификационным номером. Никакие две системы Linux, работающие на гипервизоре, не должны иметь одинаковый идентификатор машины D-Bus. 

ID машины D-Bus находится в `/var/lib/dbus/machine-id` и символически связан с `/etc/machine-id`. Изменять этот идентификационный номер в работающей системе не рекомендуется, так как возможна нестабильность системы и сбои. Если две виртуальные машины имеют одинаковый идентификатор машины D-Bus, выполните следующую процедуру, чтобы сгенерировать новый:

```console
$ sudo rm -f /etc/machine-id
$ sudo dbus-uuidgen --ensure=/etc/machine-id
```

Если `/var/lib/dbus/machine-id` не является символической ссылкой на `/etc/machine-id`, тогда `/var/lib/dbus/machine-id` необходимо будет удалить.


## Развертывание виртуальных машин в облаке

Существует множество поставщиков IaaS (*инфраструктура* как услуга), которые запускают системы гипервизоров и могут развертывать виртуальные гостевые образы для организации. Практически у всех этих поставщиков есть инструменты, которые позволяют администратору создавать, развертывать и настраивать собственные виртуальные машины на основе различных дистрибутивов Linux. Многие из этих компаний также имеют системы, позволяющие развертывать и переносить виртуальные машины, созданные внутри организации заказчика. 

При оценке развертывания системы Linux в среде IaaS есть некоторые ключевые элементы, о которых должен знать администратор: 

Вычислительные экземпляры  
Многие поставщики облачных услуг взимают плату за использование в зависимости от «вычислительных экземпляров» или того, сколько процессорного времени будет использовать ваша облачная инфраструктура. Тщательное планирование того, сколько времени на обработку действительно потребуется приложениям, поможет контролировать расходы на облачное решение. 

Вычислительные экземпляры также часто относятся к количеству виртуальных машин, выделенных в облачной среде. Опять же, большее количество экземпляров систем, работающих одновременно, также будет влиять на то, сколько общего процессорного времени будет оплачиваться организацией. 

Блочное хранилище  
У поставщиков облачных услуг также есть различные уровни блочного хранилища, доступные для использования организацией. Некоторые предложения просто предназначены для использования в качестве сетевого сетевого хранилища для файлов, а другие предложения относятся к внешнему хранилищу для виртуальной машины, выделенной из облака, для использования для размещения файлов. 

Стоимость таких предложений будет варьироваться в зависимости от объема используемого хранилища и скорости хранилища в центрах обработки данных поставщика. Более быстрый доступ к хранилищу обычно будет стоить дороже, и, наоборот, данные «в состоянии покоя» (как в архивном хранилище) часто очень недорогие. 

Сети  
Одна из основных составляющих работы с поставщиком облачных решений - это настройка виртуальной сети. Многие провайдеры IaaS будут иметь ту или иную форму веб-утилит, которые можно использовать для проектирования и реализации различных сетевых маршрутов, подсетей и конфигураций брандмауэра. Некоторые даже предоставляют решения DNS, чтобы общедоступные FQDN (*полные доменные имена*) могли быть назначены вашим системам, выходящим в Интернет. Существуют даже «гибридные» решения, которые могут подключать существующую локальную сетевую инфраструктуру к облачной инфраструктуре с помощью VPN (*виртуальной частной сети*), таким образом связывая две инфраструктуры вместе.

